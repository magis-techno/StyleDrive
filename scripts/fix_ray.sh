#!/bin/bash

# Rayä¿®å¤è„šæœ¬ - è‡ªåŠ¨è¯Šæ–­å’Œä¿®å¤Rayç›¸å…³é—®é¢˜
# Usage: ./scripts/fix_ray.sh

echo "ğŸ”§ StyleDrive Rayä¿®å¤å·¥å…·"
echo "=========================="

# é¢œè‰²å®šä¹‰
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# æ—¥å¿—å‡½æ•°
log_info() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

log_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

log_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# æ£€æŸ¥æ“ä½œç³»ç»Ÿ
check_os() {
    log_info "æ£€æŸ¥æ“ä½œç³»ç»Ÿ..."
    if [[ "$OSTYPE" == "msys" || "$OSTYPE" == "cygwin" ]]; then
        log_warning "æ£€æµ‹åˆ°Windowsç³»ç»Ÿï¼ŒRayåœ¨Windowsä¸Šå¯èƒ½ä¸ç¨³å®š"
        echo "å»ºè®®ä½¿ç”¨WSL2æˆ–Linuxè™šæ‹Ÿæœº"
        return 1
    else
        log_success "Linux/Unixç³»ç»Ÿï¼Œé€‚åˆè¿è¡ŒRay"
        return 0
    fi
}

# 1. åœæ­¢æ‰€æœ‰Rayè¿›ç¨‹
stop_ray() {
    log_info "åœæ­¢ç°æœ‰Rayè¿›ç¨‹..."
    
    # å°è¯•ä¼˜é›…åœæ­¢
    if command -v ray &> /dev/null; then
        ray stop --force 2>/dev/null
        log_success "Rayè¿›ç¨‹å·²åœæ­¢"
    else
        log_warning "Rayå‘½ä»¤æœªæ‰¾åˆ°ï¼Œå¯èƒ½æœªå®‰è£…"
    fi
    
    # å¼ºåˆ¶ç»ˆæ­¢Rayè¿›ç¨‹
    log_info "æ£€æŸ¥æ®‹ç•™Rayè¿›ç¨‹..."
    if command -v pkill &> /dev/null; then
        pkill -f "ray::" 2>/dev/null || true
        pkill -f "raylet" 2>/dev/null || true
        pkill -f "ray_" 2>/dev/null || true
        log_success "æ¸…ç†å®Œæˆ"
    fi
    
    sleep 2
}

# 2. æ¸…ç†Rayç¼“å­˜å’Œä¸´æ—¶æ–‡ä»¶
clean_ray_cache() {
    log_info "æ¸…ç†Rayç¼“å­˜å’Œä¸´æ—¶æ–‡ä»¶..."
    
    # æ¸…ç†ç”¨æˆ·ç›®å½•ä¸‹çš„Rayæ–‡ä»¶
    if [ -d "$HOME/.ray" ]; then
        rm -rf "$HOME/.ray"
        log_success "æ¸…ç† ~/.ray"
    fi
    
    # æ¸…ç†ä¸´æ—¶ç›®å½•ä¸‹çš„Rayæ–‡ä»¶
    if [ -d "/tmp/ray" ]; then
        rm -rf /tmp/ray*
        log_success "æ¸…ç† /tmp/ray*"
    fi
    
    # Windowsä¸‹çš„ä¸´æ—¶æ–‡ä»¶æ¸…ç†
    if [[ "$OSTYPE" == "msys" || "$OSTYPE" == "cygwin" ]]; then
        if [ -d "/c/Users/$USER/AppData/Local/Temp/ray" ]; then
            rm -rf "/c/Users/$USER/AppData/Local/Temp/ray"*
            log_success "æ¸…ç†Windowsä¸´æ—¶Rayæ–‡ä»¶"
        fi
    fi
}

# 3. æ£€æŸ¥ç«¯å£å ç”¨
check_ports() {
    log_info "æ£€æŸ¥Rayé»˜è®¤ç«¯å£å ç”¨..."
    
    local ports=(6379 10001 8265)
    for port in "${ports[@]}"; do
        if command -v netstat &> /dev/null; then
            if netstat -tulpn 2>/dev/null | grep ":$port " > /dev/null; then
                log_warning "ç«¯å£ $port è¢«å ç”¨"
                # å°è¯•æ‰¾åˆ°å ç”¨è¿›ç¨‹
                local pid=$(netstat -tulpn 2>/dev/null | grep ":$port " | awk '{print $7}' | cut -d'/' -f1 | head -1)
                if [ ! -z "$pid" ] && [ "$pid" != "-" ]; then
                    log_info "å ç”¨è¿›ç¨‹PID: $pid"
                    # å¯é€‰ï¼šè‡ªåŠ¨ç»ˆæ­¢å ç”¨è¿›ç¨‹ï¼ˆè°¨æ…ä½¿ç”¨ï¼‰
                    # kill -9 $pid 2>/dev/null || true
                fi
            else
                log_success "ç«¯å£ $port å¯ç”¨"
            fi
        fi
    done
}

# 4. æ£€æŸ¥ç³»ç»Ÿèµ„æº
check_resources() {
    log_info "æ£€æŸ¥ç³»ç»Ÿèµ„æº..."
    
    # æ£€æŸ¥å†…å­˜
    if command -v free &> /dev/null; then
        local mem_total=$(free -m | awk 'NR==2{printf "%.1f", $2/1024}')
        local mem_avail=$(free -m | awk 'NR==2{printf "%.1f", $4/1024}')
        log_info "æ€»å†…å­˜: ${mem_total}GB, å¯ç”¨: ${mem_avail}GB"
        
        if (( $(echo "$mem_avail < 2.0" | bc -l) )); then
            log_warning "å¯ç”¨å†…å­˜ä¸è¶³2GBï¼ŒRayå¯èƒ½è¿è¡Œä¸ç¨³å®š"
        fi
    fi
    
    # æ£€æŸ¥CPUæ ¸å¿ƒæ•°
    local cpu_cores=$(nproc 2>/dev/null || echo "unknown")
    log_info "CPUæ ¸å¿ƒæ•°: $cpu_cores"
}

# 5. é‡æ–°å¯åŠ¨Ray
start_ray() {
    log_info "å¯åŠ¨Rayé›†ç¾¤..."
    
    if ! command -v ray &> /dev/null; then
        log_error "Rayæœªå®‰è£…ï¼Œè¯·å…ˆå®‰è£…: pip install ray"
        return 1
    fi
    
    # å°è¯•å¯åŠ¨Ray
    local ray_output
    ray_output=$(ray start --head --port=6379 --object-manager-port=8076 --disable-usage-stats 2>&1)
    local ray_exit_code=$?
    
    if [ $ray_exit_code -eq 0 ]; then
        log_success "Rayé›†ç¾¤å¯åŠ¨æˆåŠŸ"
        echo "$ray_output" | grep -E "(Dashboard|Local node IP)"
    else
        log_error "Rayå¯åŠ¨å¤±è´¥"
        echo "$ray_output"
        return 1
    fi
    
    sleep 3
}

# 6. éªŒè¯RayçŠ¶æ€
verify_ray() {
    log_info "éªŒè¯RayçŠ¶æ€..."
    
    if ray status &> /dev/null; then
        log_success "Rayé›†ç¾¤è¿è¡Œæ­£å¸¸"
        ray status
        return 0
    else
        log_error "Rayé›†ç¾¤çŠ¶æ€å¼‚å¸¸"
        return 1
    fi
}

# 7. åˆ›å»ºRay workerå’Œé…ç½®
create_low_resource_config() {
    log_info "åˆ›å»ºRay workerå’Œé…ç½®æ–‡ä»¶..."
    
    local config_dir="navsim/planning/script/config/common/worker"
    local worker_dir="navsim/planning/utils/multithreading"
    
    if [ ! -d "$config_dir" ]; then
        log_error "é…ç½®ç›®å½•ä¸å­˜åœ¨: $config_dir"
        return 1
    fi
    
    if [ ! -d "$worker_dir" ]; then
        log_error "Workerç›®å½•ä¸å­˜åœ¨: $worker_dir"
        return 1
    fi
    
    # åˆ›å»ºæ–°çš„Ray workerç±»
    cat > "$worker_dir/worker_ray_existing.py" << 'EOF'
"""
Ray worker that connects to existing Ray cluster without reinitializing.
"""
import logging
from concurrent.futures import Future
from typing import Any, Iterable, List, Optional, Union

import ray
from psutil import cpu_count

from nuplan.planning.utils.multithreading.ray_execution import ray_map
from nuplan.planning.utils.multithreading.worker_pool import Task, WorkerPool, WorkerResources

logger = logging.getLogger(__name__)

class RayExistingCluster(WorkerPool):
    """
    Ray worker that connects to an existing Ray cluster without reinitializing.
    """

    def __init__(
        self,
        master_node_ip: Optional[str] = None,
        threads_per_node: Optional[int] = None,
        debug_mode: bool = False,
        log_to_driver: bool = True,
        logs_subdir: str = "logs",
        use_distributed: bool = False,
    ):
        """
        Initialize Ray worker for existing cluster.
        
        :param master_node_ip: Not used, kept for compatibility
        :param threads_per_node: Not used, kept for compatibility  
        :param debug_mode: If true, execute serially for debugging
        :param log_to_driver: If true, show logs from workers
        :param logs_subdir: Subdirectory for logs
        :param use_distributed: Not used, kept for compatibility
        """
        super().__init__()
        
        self._debug_mode = debug_mode
        self._log_to_driver = log_to_driver
        
        # Check if Ray is already initialized
        if not ray.is_initialized():
            logger.error("Ray is not initialized! Please start Ray first with: ray start --head")
            raise RuntimeError("Ray cluster not found. Please start Ray first.")
        
        logger.info("Connected to existing Ray cluster")
        
        # Get cluster resources
        cluster_resources = ray.cluster_resources()
        self._number_of_cpus = int(cluster_resources.get('CPU', cpu_count()))
        self._number_of_gpus = int(cluster_resources.get('GPU', 0))
        
        logger.info(f"Ray cluster resources - CPU: {self._number_of_cpus}, GPU: {self._number_of_gpus}")
        
        self._worker_resources = WorkerResources(
            number_of_nodes=1,  # Simplified assumption
            number_of_cpus_per_node=self._number_of_cpus,
            number_of_gpus_per_node=self._number_of_gpus,
        )

    @property
    def worker_resources(self) -> WorkerResources:
        """Inherited, see superclass."""
        return self._worker_resources

    def _map(self, task: Task, *item_lists: Iterable[List[Any]], verbose: bool = False) -> List[Any]:
        """Inherited, see superclass."""
        del verbose  # Not used
        if self._debug_mode:
            logger.info("Running in debug mode (serial execution)")
            # Execute serially for debugging
            if len(item_lists) == 1:
                return [task.fn(arg) for arg in item_lists[0]]
            else:
                return [task.fn(*args) for args in zip(*item_lists)]
        else:
            logger.info(f"Executing {len(item_lists[0])} tasks on Ray cluster")
            return ray_map(task, *item_lists)

    def map(self, func: Any, argument_list: List[Any]) -> List[Any]:
        """Inherited, see superclass."""
        if self._debug_mode:
            logger.info("Running in debug mode (serial execution)")
            return [func(arg) for arg in argument_list]
        else:
            logger.info(f"Executing {len(argument_list)} tasks on Ray cluster")
            return ray_map(func, argument_list)

    def submit(self, task: Task, *args: Any, **kwargs: Any) -> Future:
        """Inherited, see superclass."""
        if self._debug_mode:
            # Execute synchronously for debugging
            future = Future()
            try:
                result = task(*args, **kwargs)
                future.set_result(result)
            except Exception as e:
                future.set_exception(e)
            return future
        else:
            # Submit to Ray
            ray_future = ray.remote(task).remote(*args, **kwargs)
            # Convert Ray future to standard Future
            future = Future()
            
            def _get_result():
                try:
                    result = ray.get(ray_future)
                    future.set_result(result)
                except Exception as e:
                    future.set_exception(e)
            
            # Start background thread to get result
            import threading
            threading.Thread(target=_get_result, daemon=True).start()
            return future

    def __enter__(self):
        """Inherited, see superclass."""
        return self

    def shutdown(self) -> None:
        """
        Shutdown the worker and clear memory.
        Note: We don't shutdown Ray as it's an existing cluster.
        """
        logger.info("Worker shutdown (Ray cluster remains running)")
        pass

    def __exit__(self, exc_type, exc_val, exc_tb):
        """Inherited, see superclass."""
        # Don't shutdown Ray as it's an existing cluster
        logger.info("Disconnecting from Ray cluster (cluster remains running)")
        self.shutdown()
EOF
    
    log_success "åˆ›å»ºRay workerç±»: $worker_dir/worker_ray_existing.py"
    
    # åˆ›å»ºè¿æ¥ç°æœ‰é›†ç¾¤çš„é…ç½®
    cat > "$config_dir/ray_existing_cluster.yaml" << 'EOF'
_target_: navsim.planning.utils.multithreading.worker_ray_existing.RayExistingCluster
_convert_: 'all'
master_node_ip: null          # ä¸éœ€è¦ï¼Œè‡ªåŠ¨æ£€æµ‹ç°æœ‰é›†ç¾¤
threads_per_node: null        # ä¸éœ€è¦ï¼Œä½¿ç”¨ç°æœ‰é›†ç¾¤èµ„æº
debug_mode: false             # å¦‚æœä¸ºtrueåˆ™ä¸²è¡Œæ‰§è¡Œç”¨äºè°ƒè¯•
log_to_driver: true           # æ˜¾ç¤ºworkeræ—¥å¿—
logs_subdir: 'logs'           # æ—¥å¿—å­ç›®å½•
use_distributed: false        # ä¸éœ€è¦ï¼Œè¿æ¥ç°æœ‰é›†ç¾¤
EOF
    
    # åˆ›å»ºä½èµ„æºæ–°é›†ç¾¤é…ç½®ï¼ˆå¤‡ç”¨ï¼‰
    cat > "$config_dir/ray_low_resource_new.yaml" << 'EOF'
_target_: navsim.planning.utils.multithreading.worker_ray_no_torch.RayDistributedNoTorch
_convert_: 'all'
master_node_ip: null
threads_per_node: 2       # é™ä½çº¿ç¨‹æ•°å‡å°‘èµ„æºæ¶ˆè€—
debug_mode: false
log_to_driver: true
logs_subdir: 'logs'
use_distributed: false
EOF
    
    log_success "åˆ›å»ºRayé…ç½®æ–‡ä»¶"
}

# 8. åˆ›å»ºRayç¼“å­˜è„šæœ¬
create_ray_caching_script() {
    log_info "åˆ›å»ºRayç¼“å­˜è„šæœ¬..."
    
    # åˆ›å»ºè¿æ¥ç°æœ‰é›†ç¾¤çš„è„šæœ¬
    cat > "scripts/caching/caching_training_ray_existing.sh" << 'EOF'
#!/bin/bash

# StyleDrive Dataset Caching Script (Ray Existing Cluster)
# This script connects to an already running Ray cluster

# Ensure environment variables are set
if [ -z "$NAVSIM_DEVKIT_ROOT" ]; then
    echo "Error: NAVSIM_DEVKIT_ROOT is not set. Please run: source ./setup_env.sh"
    exit 1
fi

if [ -z "$NAVSIM_EXP_ROOT" ]; then
    echo "Error: NAVSIM_EXP_ROOT is not set. Please run: source ./setup_env.sh"
    exit 1
fi

echo "Using NAVSIM_DEVKIT_ROOT: $NAVSIM_DEVKIT_ROOT"
echo "Using NAVSIM_EXP_ROOT: $NAVSIM_EXP_ROOT"
echo "Using existing Ray cluster"

# Check if Ray is running
if ! ray status >/dev/null 2>&1; then
    echo "Error: Ray cluster is not running. Please start Ray first:"
    echo "  ray start --head --disable-usage-stats"
    exit 1
fi

echo "Ray cluster is running:"
ray status

# Create cache directory
mkdir -p "$NAVSIM_EXP_ROOT/training_cache"

# Run caching command with existing Ray cluster
python $NAVSIM_DEVKIT_ROOT/planning/script/run_dataset_caching.py \
    agent=diffusiondrive_style_agent \
    experiment_name=training_diffusiondrive_style_agent \
    train_test_split=styletrain \
    cache_path=$NAVSIM_EXP_ROOT/training_cache \
    worker=ray_existing_cluster
EOF

    chmod +x "scripts/caching/caching_training_ray_existing.sh"
    log_success "åˆ›å»ºRayç¼“å­˜è„šæœ¬: scripts/caching/caching_training_ray_existing.sh"
}

# ä¸»å‡½æ•°
main() {
    echo "å¼€å§‹Rayä¿®å¤æµç¨‹..."
    echo ""
    
    # æ£€æŸ¥æ“ä½œç³»ç»Ÿ
    check_os
    
    # æ­¥éª¤1: åœæ­¢Ray
    stop_ray
    
    # æ­¥éª¤2: æ¸…ç†ç¼“å­˜
    clean_ray_cache
    
    # æ­¥éª¤3: æ£€æŸ¥ç«¯å£
    check_ports
    
    # æ­¥éª¤4: æ£€æŸ¥èµ„æº
    check_resources
    
    # æ­¥éª¤5: å¯åŠ¨Ray
    if start_ray; then
        # æ­¥éª¤6: éªŒè¯çŠ¶æ€
        if verify_ray; then
            log_success "ğŸ‰ Rayä¿®å¤æˆåŠŸï¼"
        else
            log_warning "Rayå¯åŠ¨ä½†çŠ¶æ€å¼‚å¸¸ï¼Œå»ºè®®ä½¿ç”¨ThreadPoolæ¨¡å¼"
        fi
    else
        log_error "Rayå¯åŠ¨å¤±è´¥ï¼Œå»ºè®®ä½¿ç”¨ThreadPoolæ¨¡å¼"
    fi
    
    # æ­¥éª¤7: åˆ›å»ºé…ç½®
    create_low_resource_config
    
    # æ­¥éª¤8: åˆ›å»ºè„šæœ¬
    create_ray_caching_script
    
    echo ""
    echo "ğŸ”§ ä¿®å¤å®Œæˆï¼ä½¿ç”¨å»ºè®®ï¼š"
    echo "1. Rayå·²å¯åŠ¨ï¼Œä½¿ç”¨ç°æœ‰é›†ç¾¤: ./scripts/caching/caching_training_ray_existing.sh"
    echo "2. å¦‚æœä»æœ‰é—®é¢˜ï¼Œä½¿ç”¨ThreadPool: ./scripts/caching/caching_training_threadpool.sh"
    echo "3. æ£€æŸ¥RayçŠ¶æ€: ray status"
    echo "4. åœæ­¢Ray: ray stop"
}

# è¿è¡Œä¸»å‡½æ•°
main "$@"